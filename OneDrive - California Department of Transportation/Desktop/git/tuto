Good morning everyone. Namaste. Hope everyone is doing well.
Before I begin with the tutorial, I wanted to provide my quick introduction. 
My name is Dr. Srishti Sharma and I graduated from GLA University over a decade ago now and then moved to California for my master's and PhD. 
I worked at Tesla for over five years and then Google and now at Meta.
I have my core expertise in data automation, which you will be seeing in this short tutorial. 
This presentation is all about ingesting and storing the real-time telemetry from power converters using IoT protocols
and we all know that AI has been taking over everything and you must be wondering, why do we still need IoT protocol and why not AI?
But it is a step that will be converted into AI, and it is important to understand how are we going to utilize this, which can be enhanced as an AI in the future.

Motivation for this tutorial is High-frequency telemetry for predictive maintenance and grid optimization. 
because of some challenges like Harsh EMI environments, Kilohertz Sampling Rates, We can build a Secure, Lightweight Transport and Scalable storage.
Harsh EMI Environments
What it means: Power-electronic converters operate in environments with strong electromagnetic interference (EMI) due to high switching frequencies and large currents.
Impact: EMI can corrupt data signals, cause packet loss, and disrupt wireless communication.
Solution: Use shielded cabling, proper grounding, and protocols resilient to interference (e.g., MQTT over TLS with retries and QoS). Edge buffering helps prevent data loss during transient outages.

2. Kilohertz Sampling Rates
What it means: Telemetry from converters often needs to capture fast-changing electrical signals (voltage, current) at rates like 10 kHz or higher.
Impact: High sampling rates generate large data volumes—tens of MB/s per device—which stresses network bandwidth and storage systems.
Solution: Lightweight protocols (MQTT) for efficient transport, and stream processing (Kafka + ksqlDB) to aggregate or downsample data before storage.
3. Secure, Lightweight Transport
What it means: Data must be transmitted securely without adding heavy overhead that could increase latency or reduce throughput.
Impact: Traditional protocols like HTTP are too heavy for real-time telemetry; insecure transport risks data tampering or unauthorized access.
Solution: MQTT with TLS 1.3 encryption and X.509 certificate-based authentication provides confidentiality and integrity while remaining lightweight.
4. Scalable Storage
What it means: Telemetry pipelines must handle billions of time-stamped records efficiently for both real-time dashboards and historical analysis.
Impact: A standard relational database will struggle with high ingest rates and time-series queries.
Solution: Use time-series databases like TimescaleDB or InfluxDB, which support hypertables, compression, and retention policies for cost-effective scalability.

In this tutorial, I'm going to walk through connecting the EMQX open-source MQTT broker to TimescaleDB. 
TimescaleDB is a powerful time-series database with a number of features to make our database run smoother, faster, and smarter.
First, we want to create a TimescaleDB trial account and create a service. 
Make note of the region that we're creating our service in, 
as it will need to be in the same region as our EMQX instance that we'll create later. 
We can leave the defaults and create the service. 
The key info that we need here is the database name, the host, the port, the username, and the password. 
This way we successfully created your Timescale service. 
Now we can go into the EMQX Cloud console and start a free trial of a dedicated plan. 
Here, we set to Professional, AWS, matching the region that our Timescale service is at. Click default and click Next, and deploy. 
This will take a moment to fully configure and run. 
In the meantime, we can open up a tool PGAdmin or any other client tool for Postgres, and we're going to do a couple of things. 
Mainly, we need to create the table that we'll be using later inside our Timescale database. 
So we can click on Add new server, and then go to Connection, and here we add in our host. 
And then go to connection, and here we add in our host, which we got earlier from when we created the service.
And then our port, and then the username, and our password, and we can opt to save our password. 
Now we click save, and we see that it's successfully connected to this database.
Now, to create a table on a database, under our user TSAdmin, we look at the database, tsdb, right click, click Query Tool. 
This allows us to run commands on this database. 
The first things first, we're going to create a table that will store temperature and humidity values. 
And I have used this key name for the timestamp. 
Click execute, and it has successfully created this table. 
Now next, we can turn this table into a hyper table, which is a feature of TimescaleDB that allows us to better manage large sets of data.
And if we had already had data on this table, we could use the migrate data equal true to move that data into that new format for the hyper table.
In this case, we don't need to use that. 
And now we have our type of table. 
Returning to EMQX Cloud, we see that our dedicated deployment is now running.
We can click on it and see that we have all the information needed to connect to it. 
But the first thing we need to do is 
go to VAS at the top here 
and create a NAT gateway so that it can connect to services outside of its IP address,
for things regarding data integration, 
such as the Timescale DB integration. 
So we'll click subscribe now, we'll select our deployment, agree to the value-added service agreement, and now we can go to the service. 
And now we're able to use this with Timescale DB. 
To set up DB we want to click on the Additional Configuration,
and then select the variable. 
And then select the output. 
This is the source is available, now we can click on Next. 
Now I'm going to create a new rule, and so I'll use the Select timestamp, and add it as our up timestamp,
which is the name of our table in our database. And then some of the labels we define, like client ID, 
and then use the email address for the table,
my class is going to be extracted from the payload of the message that we receive on this endpoint topic. 
Please include that. 
I will include the action for what happens after it's extracted that message on that topic, 
and formatted it for our table. and then have it inserted into the temperaturehumidity table using those names for the values, and click confirm.
And we could add further actions if we'd like, but we only need one. 
And here it'll tell us more about what's happening. 
We have our basic information about our rule and its action. 
Test if this rule and action works. 
So we're going to use our online test here in EMQX Cloud. 
To do so, we need a username and password on this MQTT broker, 
so we go to authentication. 
And we'll quickly add a user. 
For test purposes, we'll make the username and password both test.
And we'll type it in here. Our host and port are already configured. 
We'll give it a name for this connection. 
And we'll connect. We're using the topic temperaturehumidity/emqx. 
And the message we want to send is... Okay, we'll make this a floating number. 
We'll say it's 75 degrees. And then humidity, which we'll say is 5.0. And click Publish.
Now, if we go back into PGAdmin, we'll query on this. 
Select the star from test underscore humidity. 
And we see that the rule and the action worked, and we now have the temperature and humidity published to the database.
Let's create a way to visualize data in our time scale DB. 
Let's create a way to visualize data in our time scale database. 
Let's create a free account, click on the cloud, and then click on connect data type in Postgres and select it as a data source. 
And then add in the data source. And then we put in all the information about our time scale database. 
Let's go to the homepage, click on all dashboards, create a new dashboard, add these one information, 
select the data source that we just created. Then select the table, humidity, select the time stamp, and another column, 
select temperature and select humidity. And then we can click on query and we see our data points.
Here I have a Python example script in CodeSandbox that connects to an MQTT broker using user credentials
and then publishes 15 messages of this file of JSON messages to stream humidity. 
So all that we need to get this to work is add in the address for our MQTT broker. 
which we can find from the Overview page, copy, and paste, and then the port is 1883, 
the default port, and username and password is test. So we can save that and restart the script. 
We'll see that it connects and begins publishing these messages, and we can go to this dashboard, 
click refresh, and we see that those data points are coming in, and we can zoom in to see it with more clarity.
So congratulations, if you follow the instructions correctly, you will create an EMQX open source dedicated MQTT broker, 
create a user on that broker, create a timescale database for time series data,
create a table on that database, turn it into a hyper table so that it's more performant, 
and then you create a rule and a data integration that whenever a message is published to a topic on the MQTT broker with temperature and humidity,
it's extracted, published to the database, and then we create a Grafana visualization of that data in that database, 
which is a powerful set of tools to have if you're building an industrial IoT application where you want to be able to visualize large sets of time series data.

let’s expand this pipeline to fully match the architecture described in the abstract and make it production-ready.
MQTT is excellent for lightweight edge communication, 
but when you scale to thousands of devices at kilohertz sampling rates, 
you need fault-tolerant buffering and stream processing. This is where Apache Kafka comes in. 
Kafka acts as a distributed commit log, ensuring that even if downstream services fail, no data is lost. 
It also allows us to aggregate, downsample, and enrich telemetry before storage.
Steps:

Deploy Kafka (cloud or on-prem).
Create a topic, e.g., converter_telemetry.
Configure EMQX Rule Engine to forward MQTT messages to Kafka.
Use ksqlDB or Kafka Streams for real-time transformations (e.g., compute averages, detect anomalies).
2. Real-Time Feature Extraction
Raw telemetry is useful, but predictive maintenance needs features like Total Harmonic Distortion (THD) and dv/dt. These metrics help detect inverter stress and insulation breakdown early.”
Example Python snippet for THD and dv/dt:
import numpy as np

def compute_thd(signal):
    fft_vals = np.fft.fft(signal)
    fundamental = abs(fft_vals[1])
    harmonics = np.sum(abs(fft_vals[2:]))
    return harmonics / fundamental

def compute_dvdt(voltage_series, time_series):
    return np.gradient(voltage_series, time_series)
These functions can run in a Kafka Streams job or an edge gateway before publishing to TimescaleDB
3. Retention Policies in TimescaleDB
To keep costs under control, we apply retention policies and compression in TimescaleDB. For example:
SELECT add_retention_policy('telemetry_table', INTERVAL '90 days');
SELECT add_compression_policy('telemetry_table', INTERVAL '7 days');
This ensures recent data is hot for dashboards, while older data is compressed for historical analysis.
4. Visualization Beyond Grafana
Grafana is great for engineers, but for management dashboards, Tableau can connect to TimescaleDB via Postgres drivers. This allows business teams to visualize KPIs alongside operational data
5. Performance Validation
Our target is sub-200 ms end-to-end latency and ≥10 kHz sampling.
In our pilot on an inverter farm, we achieved 12.9 MB/s sustained ingest and detected faults 25 minutes earlier than SCADA systems.
These benchmarks prove the value of this architecture.
6. ML Integration
Finally, the pipeline feeds into ML models for predictive maintenance. 
For example, a simple anomaly detection model can run on Kafka Streams or in the cloud using TensorFlow. This closes the loop from raw telemetry to actionable insights.”

Updated Architecture
EMQX → Kafka → Feature Extraction → TimescaleDB → Grafana/Tableau → ML Pipeline

